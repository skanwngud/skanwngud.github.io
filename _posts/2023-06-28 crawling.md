---
title: Tor Browser 를 활용한 외부 IP 접속 selenium 크롤링 (프록시 관련 내용 추후 추가)
author: yjh
math: True
categories: [python]
tags: [python, crawling]
---
## 프록시 서버 (Proxy server)
- 인터넷 접속 시 *빠른 엑세스*나 *안전한 통신*을 확보하기 위한 **중계서버**
- 클라이언트와 서버의 중간에 위치하고 있어 *대신 통신을 받아주는 서버*


## Tor Browser install
```bash
# tor browser install
sudo apt-get update
sudo apt-get install tor

# python 으로 tor browser 를 사용하기 위한 stem 설치
sudo apt-get install python3-stem

# tor password 설정
tor --hash-password [password]

# tor config 수정
vim /etc/tor/torrc
    # ControlPort9051 uncomment
    # HashedControlPassword uncomment 그리고 위에서 나온 password 붙여넣기 (16:~ 뒤로)

# tor restart
sudo service tor restart

# chrome driver install
sudo apt-get install chromium-chromdriver
```

## Python crawling code
```python
# 필요 라이브러리 임포트
import time
import urllib.request
import os

from random import randrange

from stem import Signal
from stem.control import Controller

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.firefox.options import Options

from urllib.parse import quote_plus


def function_check(test_url: str) -> None:
    # tor browser 의 기능 동작 체크
    with Controller.from_port(port=9051) as controller:
        while True:
            url = test_url
            hostname = "socks5://127.0.0.1"
            port = "9050"

            chrome_options = webdriver.ChromeOptions()
            chrome_options.add_argument(f"--proxy-server={hostname + ':' + port}")
            
            driver = webdriver.Chrome(options=chrome_options)

            driver.get(url)
            time.sleep(20)
            driver.quit()

            controller.authenticate("password for tor browser")
            controller.signal(Signal.NEWNYM)

            if controller.is_newnym_available() == False:
                print(f"waiting for Tor to Change IP: {controller.get_newnym_wait()} sec")
                time.sleep(controller.get_newnym_wait())


# 크롤러 클래스 정의
class Crawler:
    def __init__(self, url: str, query: str, max_images: int=None) -> None:
        self.__mode = "google" if "google" in url else "naver"
        self.__url = url if self.__mode == "google" else url + quote_plus(query)
        self.__query = query
        self.__max_images = max_images

        options = Options()
        options.set_preference("network.proxy.type", 1)
        options.set_preference("network.proxy.socks", "127.0.0.1")
        options.set_preference("network.proxy.socks_port", 9050)

        opener = urllib.request.build_opner()
        opener.addheaders = [('Users-Agents', 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]

        urllib.request.install_opener(opener)

        self.driver = webdriver.Firefox(options=options)
        self.driver.get("http://icanhazip.com/")  # 단순 ip 확인용 웹사이트

        current_ip = self.driver.page_source.split("<pre>")[-1].split("\n")[0]  # 현재 접속 IP

        # 현재 접속 IP 와 본래의 IP 와 일치한다면 바로 종료
        if current_ip == "###.###.###.###":
            logger.error(f"changed ip is same as original ip (###.###.###.### == {current_ip})")
            return

        self.new_tab()

    def new_tab(self) -> None:
        # ip 확인용 사이트가 아닌 실제 크롤링을 실시할 탭 열기
        tab = self.driver.find_element(By.TAG_NAME, "body")
        tab.send_keys(Keys.CONTROL + "t")

        self.driver.get(self.__url)

        # 접속 IP 가 바뀌게 되면 뭔가를 Accept 하라는 창이 뜨는데 해당 창의 Accept 태그
        try:
            self.driver.find_element(By.ID, "L2AGLb").click()

        except:
            pass

    def search_image(self) -> None:
        # google 의 경우 검색창에 크롤링 할 검색어 입력 후 처리
        search_bar = self.driver.find_element(By.NAME, "q")
        search_bar.send_keys(self.__query)
        search_bar.submit()

    def load_image(self) -> None:
        # 검색 된 이미지 전체 로드
        last_height = self.driver.execute("return document.body.scrollHeight;")

        while True:
            # 아래로 스크롤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

            time.sleep(randrange(10, 14))  # 페이지가 로드 될 때까지 기다림

            new_height = self.driver.execute_script("return document.body.scrollHeight;")

            if new_height == last_height:
                try:
                    # 일정수준 이미지가 로드 되면 show more image 등이 뜨며 클릭버튼이 생기는데 해당 버튼을 클릭해 추가 이미지들을 로드
                    more_images = self.driver.find_element(By.CSS_SELECTOR, ".mye4qd")
                    more_images.click()

                except:
                    break

            last_height = new_height

    def get_images_google(self) -> None:
        # google 에서 크롤링 할 때
        time.sleep(randrange(10, 15))
        images = self.driver.find_elements(By.CSS_SELECTOR, ".rg_i")

        for idx, img in enumerate(images[:self.__max_images]):
            try:
                img.click()
                time.sleep(randrange(7, 10))

                logger.info(f"{idx} / {len(images)}")

                img_url = self.driver.find_element(By.XPATH, "/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div[2]/div[2]/div[2]/div[2]/c-wiz/div/div/div/div[3]/div[1]/a/img[1]").get_attribute("src")
                urllib.request.urlretrieve(img_url, f"image/{time.time()}.jpg")

            except Exception as e:
                logger.error(e)
                pass

    def get_images_naver(self) -> None:
        # naver 에서 크롤링 할 때
        time.sleep(randrange(10, 14))
        images = self.driver.find_elements(By.CLASS_NAME, "_image")
        
        for idx, img in enumerate(images[:self.__max_images]):
            try:
                img.click()
                time.sleep(randrange(3, 7))

                logger.info(f"{idx} / {len(images)}")

                img_url = self.driver.find_element(By.XPATH, '//*[@id="main_pack"]/section[2]/div/div[2]/div/div/div[1]/div[1]/div/div/div[1]/div[1]/img').get_attribute("src")

                urllib.request.urlretrieve(img_url, f"images/{time.time()}.jpg")

            except Exception as e:
                logger.error(e)
                pass

    def run(self) -> None:
        os.makedirs("images", exist_ok=True)

        if self.__mode == "google":
            self.search_image()
            self.load_image()
            self.get_images_google()

        else:
            self.load_image()
            self.get_images_naver()

    def __del__(self) -> None:
        self.driver.quit()


if __name__ == "__main__":
    google = Crawler("https://www.google.com/imghp?hl=en&tab=ri&authuser=0&ogbl", "과자 유통기한", 100)
    google.run()

    naver = Crawler("https://search.naver.com/search.naver?where=image&section=image&query=", "과자 유통기한", 100)
    naver.run()

```

[Reference]
- https://real1004zz.tistory.com/m/224